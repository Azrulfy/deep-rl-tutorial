{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Tutorial on Policy Gradient Methods\n",
    "# Author: Robert T Lange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rtl/anaconda2/envs/ma-vision/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.77561978, -0.63120041,  0.3537931 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.77841584, -0.62774897, -0.08883786]), -6.056745576278789, False, {})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "action = np.array([0.205129])\n",
    "\n",
    "low = env.action_space.low\n",
    "high = env.action_space.high\n",
    "\n",
    "a = np.clip(action, low, high)\n",
    "env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = [np.array([0.2323]), np.array([0.34343])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.64023891, -0.76817585,  0.08822884],\n",
       "       [ 0.97492677, -0.22252596,  0.84514401]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from drl_toolbox.single_rl import make_parallel_env\n",
    "env = make_parallel_env(\"Pendulum-v0\",\n",
    "                        0, 2)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0, 1])\n",
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-91066be186ba>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-91066be186ba>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    env.step([)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "env.step([)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.2323]), array([0.34343])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(0), array(1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.array(a_t) for a_t in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "def worker(remote, parent_remote, env_fn_wrapper):\n",
    "    parent_remote.close()\n",
    "    env = env_fn_wrapper.x()\n",
    "    while True:\n",
    "        cmd, data = remote.recv()\n",
    "        if cmd == 'step':\n",
    "            ob, reward, done, info = env.step(data)\n",
    "            if done:\n",
    "                ob = env.reset()\n",
    "            remote.send((ob, reward, done, info))\n",
    "        elif cmd == 'reset':\n",
    "            ob = env.reset()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'reset_task':\n",
    "            ob = env.reset_task()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'close':\n",
    "            remote.close()\n",
    "            break\n",
    "        elif cmd == 'get_spaces':\n",
    "            remote.send((env.observation_space, env.action_space))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "class VecEnv(object):\n",
    "    \"\"\"\n",
    "    An abstract asynchronous, vectorized environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_envs, observation_space, action_space):\n",
    "        self.num_envs = num_envs\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset all the environments and return an array of\n",
    "        observations, or a tuple of observation arrays.\n",
    "        If step_async is still doing work, that work will\n",
    "        be cancelled and step_wait() should not be called\n",
    "        until step_async() is invoked again.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        \"\"\"\n",
    "        Tell all the environments to start taking a step\n",
    "        with the given actions.\n",
    "        Call step_wait() to get the results of the step.\n",
    "        You should not call this if a step_async run is\n",
    "        already pending.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step_wait(self):\n",
    "        \"\"\"\n",
    "        Wait for the step taken with step_async().\n",
    "        Returns (obs, rews, dones, infos):\n",
    "         - obs: an array of observations, or a tuple of\n",
    "                arrays of observations.\n",
    "         - rews: an array of rewards\n",
    "         - dones: an array of \"episode done\" booleans\n",
    "         - infos: a sequence of info objects\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Clean up the environments' resources.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.step_async(actions)\n",
    "        return self.step_wait()\n",
    "\n",
    "    \n",
    "class CloudpickleWrapper(object):\n",
    "    \"\"\"\n",
    "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    def __getstate__(self):\n",
    "        import cloudpickle\n",
    "        return cloudpickle.dumps(self.x)\n",
    "    def __setstate__(self, ob):\n",
    "        import pickle\n",
    "        self.x = pickle.loads(ob)\n",
    "\n",
    "        \n",
    "class SubprocVecEnv(VecEnv):\n",
    "    def __init__(self, env_fns, spaces=None):\n",
    "        \"\"\"\n",
    "        envs: list of gym environments to run in subprocesses\n",
    "        \"\"\"\n",
    "        self.waiting = False\n",
    "        self.closed = False\n",
    "        nenvs = len(env_fns)\n",
    "        self.nenvs = nenvs\n",
    "        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n",
    "        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n",
    "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
    "        for p in self.ps:\n",
    "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
    "            p.start()\n",
    "        for remote in self.work_remotes:\n",
    "            remote.close()\n",
    "\n",
    "        self.remotes[0].send(('get_spaces', None))\n",
    "        observation_space, action_space = self.remotes[0].recv()\n",
    "        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        for remote, action in zip(self.remotes, actions):\n",
    "            remote.send(('step', action))\n",
    "        self.waiting = True\n",
    "\n",
    "    def step_wait(self):\n",
    "        results = [remote.recv() for remote in self.remotes]\n",
    "        self.waiting = False\n",
    "        obs, rews, dones, infos = zip(*results)\n",
    "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
    "\n",
    "    def reset(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def reset_task(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset_task', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def close(self):\n",
    "        if self.closed:\n",
    "            return\n",
    "        if self.waiting:\n",
    "            for remote in self.remotes:            \n",
    "                remote.recv()\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('close', None))\n",
    "        for p in self.ps:\n",
    "            p.join()\n",
    "            self.closed = True\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.nenvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 3\n",
    "env_name = \"Pendulum-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns\n",
    "\n",
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 10\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = -200\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "max_frames = 10\n",
    "frame_idx  = 0\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "[tensor([[-50.4879],\n",
      "        [-30.6074],\n",
      "        [ -9.5683]], grad_fn=<AddBackward0>), tensor([[-48.6792],\n",
      "        [-31.0566],\n",
      "        [-10.0888]], grad_fn=<AddBackward0>), tensor([[-46.8133],\n",
      "        [-31.5220],\n",
      "        [-10.5692]], grad_fn=<AddBackward0>), tensor([[-44.7081],\n",
      "        [-31.7268],\n",
      "        [-10.9529]], grad_fn=<AddBackward0>), tensor([[-42.1678],\n",
      "        [-31.4641],\n",
      "        [-11.1314]], grad_fn=<AddBackward0>), tensor([[-38.8040],\n",
      "        [-30.6559],\n",
      "        [-11.1336]], grad_fn=<AddBackward0>), tensor([[-34.3036],\n",
      "        [-28.9670],\n",
      "        [-10.6924]], grad_fn=<AddBackward0>), tensor([[-28.3502],\n",
      "        [-25.7622],\n",
      "        [ -9.7362]], grad_fn=<AddBackward0>), tensor([[-20.6461],\n",
      "        [-20.4984],\n",
      "        [ -8.1192]], grad_fn=<AddBackward0>), tensor([[-11.0852],\n",
      "        [-12.5355],\n",
      "        [ -5.4365]], grad_fn=<AddBackward0>)]\n",
      "torch.Size([30, 1])\n",
      "torch.Size([30, 1])\n",
      "torch.Size([30, 1])\n",
      "tensor([[-50.4329],\n",
      "        [-30.7752],\n",
      "        [ -9.3755],\n",
      "        [-48.6794],\n",
      "        [-31.0111],\n",
      "        [ -9.8513],\n",
      "        [-46.8776],\n",
      "        [-31.2014],\n",
      "        [-10.2692],\n",
      "        [-44.8238],\n",
      "        [-31.2528],\n",
      "        [-10.5405],\n",
      "        [-42.3125],\n",
      "        [-30.9059],\n",
      "        [-10.6627],\n",
      "        [-38.9773],\n",
      "        [-29.9845],\n",
      "        [-10.5372],\n",
      "        [-34.5070],\n",
      "        [-28.1098],\n",
      "        [-10.0100],\n",
      "        [-28.5870],\n",
      "        [-24.7333],\n",
      "        [ -8.9654],\n",
      "        [-20.9148],\n",
      "        [-19.3070],\n",
      "        [ -7.2215],\n",
      "        [-11.3773],\n",
      "        [-11.2190],\n",
      "        [ -4.3901]])\n"
     ]
    }
   ],
   "source": [
    "state = envs.reset()\n",
    "early_stop = False\n",
    "\n",
    "while frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        # print(log_prob.size())\n",
    "        log_probs.append(log_prob)\n",
    "        # print(log_prob.size())\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    print(next_value.size())\n",
    "    #print(len(rewards))\n",
    "    # print(len(masks))\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "    print(returns)\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    print(log_probs.size())\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    print(values.size())\n",
    "    print(returns.size())\n",
    "    advantage = returns - values\n",
    "    print(advantage)\n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum mean entropy (over threads) for all timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ma-vision)",
   "language": "python",
   "name": "ma-vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
